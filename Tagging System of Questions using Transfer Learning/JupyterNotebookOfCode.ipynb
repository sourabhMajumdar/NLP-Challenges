{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging System of Questions using Transfer Learning\n",
    "\n",
    "## Overview \n",
    "\n",
    "In this challenge, we provide the titles, text, and tags of Stack Exchange questions from six different sites. We then ask for tag predictions on unseen physics questions. Solving this problem via a standard machine learning approach might involve training an algorithm on a corpus of related text. Here, you are challenged to train on material from outside the field. Can an algorithm predict appropriate physics tags after learning from biology, chemistry or mathematics data? Let's find out!\n",
    "\n",
    "## Objective \n",
    "\n",
    "Main goal of this task is to train a model on questions belonging to domains like biology, chemistry, or mathematics but use that to predict tags of physics question.  These tags describe the topic of questions.\n",
    "\n",
    "## Dataset\n",
    "In this dataset, you are provided with question titles, content, and tags for Stack Exchange sites on a variety of topics (biology, cooking, cryptography, diy, robotics, and travel). The content of each question is given as HTML. ​The tags are words or phrases that describe the topic of the question. The test set is comprised of questions from the ​[physics.stackexchange.com](https://physics.stackexchange.com)\n",
    "​ . For each question in the test set, you should use the title and question content in order to generate potential tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Relevant libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # to create deep neural networks\n",
    "import pandas as pd # to handle the spreadsheets\n",
    "from bs4 import BeautifulSoup # to process html tags\n",
    "import os\n",
    "import glob\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "import random\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting the hyper parameters**\n",
    "\n",
    "1. *neg_sample_size* : for every positive sample, we create *neg_sample_size* negative examples.\n",
    "\n",
    "2. *embedding_size* : determines the embedding dimension of both our word embeddings and hidden layers of our recurrent neural networks\n",
    "\n",
    "3. *epochs* : determine the number of epochs for training\n",
    "\n",
    "4. *batch_size* : determines the size of batch for every training iteration\n",
    "\n",
    "5. *data_dir* : specifies the location of training data\n",
    "\n",
    "6. *test_data_dir* : specifies the location of testing data\n",
    "\n",
    "**Note**\n",
    "\n",
    "Due to time, space and computational limitations the value of epochs is set to 10, neg_sample_size to 4 and train and test data directories to sample train and sample test respectively.\n",
    "\n",
    "If you wish to conduct full training, please change the values respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sample_size = 4\n",
    "embedding_size = 128\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "data_dir = './Dataset/sample_train/'\n",
    "test_data_dir = './Dataset/sample_test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extractining data from the files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_columns = ['title','content','tags']\n",
    "data_frame = pd.DataFrame(columns=list_of_columns)\n",
    "for file in glob.glob(data_dir + '*') :\n",
    "    temp_data_frame = pd.read_csv(file)\n",
    "    data_frame = data_frame.append(temp_data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the tokenize functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text) :\n",
    "    \n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    soup = BeautifulSoup(text)\n",
    "    content = soup.get_text()\n",
    "    \n",
    "    raw_tokens = word_tokenize(content)\n",
    "    raw_tokens = [w.lower() for w in raw_tokens]\n",
    "    \n",
    "    tokens = list()\n",
    "    \n",
    "    for tk in raw_tokens :\n",
    "        tkns = tk.split('-')\n",
    "        for tkn in tkns :\n",
    "            tokens.append(tkn)\n",
    "    \n",
    "    old_punctuation = string.punctuation\n",
    "    new_punctuation = old_punctuation.replace('-','')\n",
    "    \n",
    "    table = str.maketrans('','',new_punctuation)\n",
    "    #table = str.maketrans('','',string.punctuation)\n",
    "    \n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_candidates(text) :\n",
    "    \n",
    "    initial_candidates = text.lower().split()\n",
    "    final_candidates = list()\n",
    "    \n",
    "    for candidate in initial_candidates :\n",
    "        intermidiate_candidates = candidate.split('-')\n",
    "        final_candidates.append(intermidiate_candidates)\n",
    "        \n",
    "    return final_candidates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the tokenized version of all the train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = set()\n",
    "train_set = list()\n",
    "labels_set = list()\n",
    "tags_set = set()\n",
    "\n",
    "max_sentence_length = -9999\n",
    "for index, row in data_frame.iterrows() :\n",
    "    raw_content = row['content']\n",
    "    raw_title = row['title']\n",
    "    raw_tags = row['tags']\n",
    "    \n",
    "    content_words = tokenize(raw_content)\n",
    "    title_words = tokenize(raw_title)\n",
    "    tag_words = tokenize(raw_tags)\n",
    "    \n",
    "    candidate_words = tokenize_candidates(raw_tags)\n",
    "    \n",
    "    sequence = list()\n",
    "    sequence.extend(title_words)\n",
    "    sequence.extend(content_words)\n",
    "    \n",
    "    train_set.append(sequence)\n",
    "    labels_set.append(candidate_words)\n",
    "    \n",
    "    for tag in tag_words :\n",
    "        tags_set.add(tag)\n",
    "        \n",
    "    new_set_of_words = content_words + title_words + tag_words\n",
    "    for w in new_set_of_words :\n",
    "        vocab_set.add(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the vocabulary dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = dict()\n",
    "vocab_size = len(vocab_set)\n",
    "for i,word in enumerate(vocab_set) :\n",
    "    word_idx[word] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the train data with positive and negative samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list()\n",
    "max_sentence_size = -9999\n",
    "\n",
    "\n",
    "for i in range(len(train_set)) :\n",
    "    train_x = train_set[i]\n",
    "    neg_tags = copy.deepcopy(tags_set)\n",
    "    \n",
    "    tags_to_remove = labels_set[i]\n",
    "    \n",
    "    for j in range(neg_sample_size) :\n",
    "        for tags_list in tags_to_remove :\n",
    "            for tag in tags_list :\n",
    "                if tag in neg_tags :\n",
    "                    neg_tags.remove(tag)\n",
    "                    \n",
    "        neg_tag = random.sample(neg_tags,random.randint(0,2))\n",
    "        \n",
    "        # append a negative sample\n",
    "        x = list()\n",
    "        x.extend(train_x)\n",
    "        x.extend(neg_tag)\n",
    "        \n",
    "        # finding the maximum sentence size, this will be used to set the padding limit\n",
    "        max_sentence_size = max(max_sentence_size,len(x))\n",
    "        train_data.append([x,0])\n",
    "        \n",
    "        tags_to_remove = neg_tag\n",
    "        \n",
    "    # append the true sample\n",
    "    for true_label in labels_set[i] :\n",
    "        x = list()\n",
    "        x.extend(train_x)\n",
    "        x.extend(true_label)\n",
    "        # finding the maximum sentence size\n",
    "        max_sentence_size = max(max_sentence_size,len(x))\n",
    "        train_data.append([x,1])\n",
    "random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating vectorized version of train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_train_data = list()\n",
    "vec_train_labels = list()\n",
    "\n",
    "for train_set in train_data :\n",
    "    train_seq = train_set[0]\n",
    "    train_label = train_set[1]\n",
    "    lq = max(0,max_sentence_size-len(train_seq))\n",
    "    \n",
    "    vec_seq = [word_idx[w] if w in word_idx.keys() else 0 for w in train_seq] + lq*[0]\n",
    "    \n",
    "    vec_train_data.append(vec_seq)\n",
    "    vec_train_labels.append([train_label])\n",
    "\n",
    "train_x = np.array(vec_train_data)\n",
    "train_y = np.array(vec_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the tensorflow model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.int32,[None,max_sentence_size],name=\"input_seq\")\n",
    "y = tf.placeholder(tf.int32,[None,1],name=\"answers\")\n",
    "\n",
    "A = tf.get_variable(\"word_embeddings\",[vocab_size,embedding_size])\n",
    "\n",
    "word_embeddings = tf.nn.embedding_lookup(A,x)\n",
    "\n",
    "rnn_cell = tf.nn.rnn_cell.LSTMCell(embedding_size)\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(rnn_cell,word_embeddings,dtype=tf.float32)\n",
    "dense_layer = tf.layers.dense\n",
    "logit_outputs = dense_layer(outputs[:,-1,:],1,activation=tf.nn.sigmoid)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(y,logit_outputs)\n",
    "train_op = tf.train.GradientDescentOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the tensorflow model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "init_op = tf.global_variables_initializer()\n",
    "batches = zip(range(0,len(train_x)-batch_size,batch_size),range(batch_size,len(train_x),batch_size))\n",
    "batches = [(start,end) for start, end in batches]\n",
    "session.run(init_op)\n",
    "for epoch in range(epochs) :\n",
    "    for start,end in tqdm(batches) :\n",
    "        train_batch = train_x[start:end]\n",
    "        train_ans = train_y[start:end]\n",
    "        \n",
    "        feed_dict = {x : train_batch, y : train_ans}\n",
    "        _, l = session.run([train_op,loss],feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = list()\n",
    "max_test_sentence_length = -9999\n",
    "\n",
    "\n",
    "test_list_of_columns = ['title','content']\n",
    "test_data_frame = pd.DataFrame(columns=test_list_of_columns)\n",
    "\n",
    "for file in glob.glob(data_dir + '*') :\n",
    "    temp_data_frame = pd.read_csv(file)\n",
    "    test_data_frame = test_data_frame.append(temp_data_frame)\n",
    "\n",
    "    \n",
    "for index, row in test_data_frame.iterrows() :\n",
    "    raw_content = row['content']\n",
    "    raw_title = row['title']\n",
    "    \n",
    "    content_words = tokenize(raw_content)\n",
    "    title_words = tokenize(raw_title)\n",
    "    \n",
    "    sequence = list()\n",
    "    sequence.extend(title_words)\n",
    "    sequence.extend(content_words)\n",
    "    \n",
    "    max_test_sentece_length = max(max_test_sentence_length,len(sequence))\n",
    "    \n",
    "    test_set.append(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making and Storing Predictions**\n",
    "\n",
    "For each test example, we make predictions, rank them and store the best 4 in the *submission.csv* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_test_columns = ['id','tags']\n",
    "test_data_frame = pd.DataFrame(columns=list_of_test_columns)\n",
    "tag_list = list(tags_set)\n",
    "id_count = 0\n",
    "for initial_seq in test_set :\n",
    "    test_seq_raw = initial_seq\n",
    "    score_list = list()\n",
    "    tag_identification_list = list()\n",
    "    id_count += 1\n",
    "    response_dict = dict()\n",
    "    response_dict['id'] = id_count\n",
    "    for i in tqdm(range(len(tag_list))) :\n",
    "        \n",
    "        first_tag = tag_list[i]\n",
    "        test_seq = copy.deepcopy(test_seq_raw)\n",
    "        test_seq.extend([first_tag])\n",
    "        #print(max_sentence_size)\n",
    "        lq = max(0,max_sentence_size-len(test_seq))\n",
    "        vec_seq = [word_idx[w] if w in word_idx.keys() else 0 for w in test_seq] + lq*[0]\n",
    "        test_case = np.array([vec_seq])\n",
    "        score = session.run([logit_outputs],feed_dict={x:test_case})\n",
    "        score_list.append(score[0][0])\n",
    "        tag_identification_list.append(first_tag)\n",
    "        \n",
    "        for j in range(i+1,len(tag_list)) :\n",
    "            \n",
    "            second_tag = tag_list[j]\n",
    "            test_seq = copy.deepcopy(test_seq_raw)\n",
    "            test_seq.extend([first_tag,second_tag])\n",
    "            lq = max(0,max_sentence_size-len(test_seq))\n",
    "            vec_seq = [word_idx[w] if w in word_idx.keys() else 0 for w in test_seq] + lq*[0]\n",
    "            test_case = np.array([vec_seq])\n",
    "            score = session.run([logit_outputs],feed_dict={x : test_case})\n",
    "            score_list.append(score)\n",
    "            tag_identification_list.append('{}-{}'.format(first_tag,second_tag))\n",
    "            \n",
    "    index_array = np.array(score_list)\n",
    "    sorted_index_array = np.argsort(index_array.reshape(index_array.shape[0],))\n",
    "    index_list = sorted_index_array.tolist()[-4:]\n",
    "    #print(index_array)\n",
    "    list_of_correct_responses = list()\n",
    "    for index in index_list :\n",
    "        print(tag_identification_list[index])\n",
    "        list_of_correct_responses.append(tag_identification_list[index])\n",
    "    tag_ans = ' '.join(list_of_correct_responses)\n",
    "    response_dict['tags'] = tag_ans\n",
    "    test_data_frame = test_data_frame.append(response_dict,ignore_index=True)\n",
    "    print('\\n')\n",
    "test_data_frame.to_csv('submission.csv',sep=',',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
